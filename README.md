# MoA-local

## Introduction
This is a simplified agentic workflow process based on Mixture of Agents (MoA) system by Wang et al. (2024). Mixture-of-Agents Enhances Large Language Model Capabilities.
This demo is a 3-layer MoA with 4 open-source models of Qwen2, Qwen 1.5, Mixtral, and dbrx. Qwen2 acts as the aggregator in the final layer.
